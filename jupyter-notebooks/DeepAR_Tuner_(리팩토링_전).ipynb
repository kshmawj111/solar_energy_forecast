{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepAR Tuner (리팩토링 전).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4yeejI6IAQV",
        "outputId": "0b629dfc-8a08-4c33-b9ab-ba4b7bdf2ed0"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os, sys\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "nb_path = '/content/drive/MyDrive/Colab Notebooks/site-packages'\n",
        "my_pakcage = '/content/drive/MyDrive/Colab Notebooks/my-packages'\n",
        "sys.path.insert(0, nb_path)  # or append(nb_path)\n",
        "sys.path.insert(0, my_pakcage)  # or append(nb_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEEZ-Cc7x1tN"
      },
      "source": [
        "DeepAR hyper-parameter tuner using Bayesian-optimization.\n",
        "\n",
        "Environment: tested on Google colab's gpu runtime environment. Expected to also work on cpu environment\n",
        "\n",
        "Used 3rd party packages: pandas, numpy, mxnet, gluonts\n",
        "\n",
        "Used internal packages: typing, os, pathlib, warnings\n",
        "\n",
        "Usage:\n",
        "\n",
        "    1. prepare a dataset with timestamp as index.\n",
        "\n",
        "    2. split the dataset into train and valid set.\n",
        "       The types of two datasets are recommended as pandas DataFrame. May not work on other types.\n",
        "\n",
        "    3. set the parameter bounds as dictionary with tuples or single number.\n",
        "        ex) {num_cells : (20, 40), epochs : 30, ... }\n",
        "        The parameters used are defined in the class DeepAR.model method\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XhNhYXNELT7"
      },
      "source": [
        "## 내부 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCSedritAr-D"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, Tuple, List, Union, Optional\n",
        "import mxnet as mx\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from solar_energy_forecast.utils.timestamper import Timestamper\n",
        "from solar_energy_forecast.utils.utilities import *\n",
        "\n",
        "from gluonts.model.predictor import Predictor\n",
        "from gluonts.dataset.common import Dataset\n",
        "from gluonts.model.deepar import DeepAREstimator\n",
        "from gluonts.mx.trainer import Trainer\n",
        "from gluonts.dataset.common import ListDataset\n",
        "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
        "\n",
        "try:\n",
        "    from bayes_opt import BayesianOptimization\n",
        "\n",
        "except ImportError as e:\n",
        "    print(\"Bayesian Optimization package cannot be imported. Check if it is installed.\"\n",
        "          \"If not installed use $ pip install bayesian-optimization\")\n",
        "    exit(-10)\n",
        "\n",
        "\n",
        "# abstract class for Bayesian Tuner\n",
        "class BayesianTuner:\n",
        "    def __init__(self,\n",
        "                 # input dataset format not determined\n",
        "                 train_df,\n",
        "                 valid_df,\n",
        "                 pbounds: Dict[str, Union[Tuple[float, float]]]):  # {param_name: (lower, upper), ... }\n",
        "        self._best_loss = None\n",
        "        self._predictor = None\n",
        "        self._estimator = None\n",
        "        self.train_df = train_df\n",
        "        self.valid_df = valid_df\n",
        "        self.pbounds = pbounds\n",
        "        self._records = []\n",
        "\n",
        "    # given forecast and true values, return the sum of all\n",
        "    def quantile_loss(self, y_true: Union[np.ndarray, pd.Series, pd.DataFrame],\n",
        "                      y_forecast: Union[np.ndarray, pd.DataFrame],\n",
        "                      quantiles: Optional[List[float]] = None):\n",
        "        # TODO: may modify the base calculation structure base to numpy instead of dataframe, for performance.\n",
        "        # default quantiles\n",
        "        if not quantiles:\n",
        "            quantiles = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "\n",
        "        # check if the quantiles are same\n",
        "        assert len(quantiles) == y_forecast.shape[\n",
        "            1], \"Number of quantiles from forecast and quantiles list is different\"\n",
        "\n",
        "        # cast forecasts to dataframe always\n",
        "        if isinstance(y_forecast, np.ndarray):\n",
        "            y_forecast = pd.DataFrame(y_forecast, columns=quantiles)\n",
        "\n",
        "        elif isinstance(y_forecast, pd.DataFrame):\n",
        "            y_forecast.columns = quantiles\n",
        "\n",
        "        # cast y_true\n",
        "        if isinstance(y_true, pd.DataFrame):\n",
        "            assert y_true.shape[1] != 1, \"y_true value must be shape of ( , 1)\"\n",
        "            y_true = y_true.values\n",
        "\n",
        "        elif isinstance(y_true, pd.Series):\n",
        "            y_true = y_true.values\n",
        "\n",
        "        # quantile loss = max(q*(y_pred - y_true), (1-q)*(y_pred, y_true))\n",
        "        for quantile in y_forecast.columns:\n",
        "            diff = y_forecast[quantile] - y_true\n",
        "            diff = np.where(diff >= 0, diff * quantile, (quantile - 1) * diff)\n",
        "            y_forecast[quantile] = diff\n",
        "\n",
        "        return y_forecast.sum().sum()\n",
        "\n",
        "    # abstract method\n",
        "    def model(self, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def tune_model(self, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # exception handling\n",
        "    def return_records(self) -> pd.DataFrame:\n",
        "        if self._records is not None:\n",
        "            return pd.concat(self._records)\n",
        "\n",
        "        else:\n",
        "            warnings.warn(\"You must first train the model to get trained estimator. Call tune_model first.\")\n",
        "\n",
        "    def return_estimator(self):\n",
        "        if self._estimator is not None:\n",
        "            return self._estimator\n",
        "\n",
        "        else:\n",
        "            warnings.warn(\"You must first train the model to get trained estimator. Call tune_model first.\")\n",
        "\n",
        "    def return_predictor(self):\n",
        "        if self._predictor is not None:\n",
        "            return self._predictor\n",
        "\n",
        "        else:\n",
        "            warnings.warn(\"You must first train the model to get trained predictor. Call tune_model first.\")\n",
        "\n",
        "    def return_best_loss(self):\n",
        "        if self._best_loss != 0:\n",
        "            return round(self._best_loss, 4)\n",
        "\n",
        "        else:\n",
        "            warnings.warn(\"You must first train the model to get best loss. Call tune_model first.\")\n",
        "\n",
        "class DeepARTuner(BayesianTuner):\n",
        "    def __init__(self,\n",
        "                 train_df: pd.DataFrame,\n",
        "                 valid_df: pd.DataFrame,\n",
        "                 pbounds: Dict[str, Tuple[float, float]],\n",
        "                 learning_rate: float,\n",
        "                 use_feat_dynamic_real: bool = True,\n",
        "                 prediction_window: Optional[int] = None,\n",
        "                 batch_size: int = 64):\n",
        "        super().__init__(train_df, valid_df, pbounds)\n",
        "        # check available device\n",
        "        if not mx.test_utils.list_gpus():\n",
        "            self.ctx = mx.context.cpu()\n",
        "\n",
        "        else:\n",
        "            self.ctx = mx.context.gpu()\n",
        "\n",
        "        # set prediction length\n",
        "        if prediction_window:\n",
        "            self.prediction_window = prediction_window\n",
        "\n",
        "        else:\n",
        "            self.prediction_window = 2 * 48  # two days as default\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.transform_to_ListData()\n",
        "        self.use_feat_dynamic_real = use_feat_dynamic_real\n",
        "        self.batch_size = batch_size\n",
        "        self.internal_iter_num = 0\n",
        "        self.current_saving_folder = None\n",
        "\n",
        "    # method to convert given dataframe into ListData class of gluonts\n",
        "    def transform_to_ListData(self):\n",
        "        train_DHI = self.train_df.DHI.values[:-self.prediction_window]\n",
        "        train_DNI = self.train_df.DNI.values[:-self.prediction_window]\n",
        "        train_RH = self.train_df.RH.values[:-self.prediction_window]\n",
        "        train_T = self.train_df['T'][:-self.prediction_window].values\n",
        "\n",
        "        self.train_ds = ListDataset(\n",
        "            [{\"start\": self.train_df.index[0],\n",
        "              \"target\": np.array(self.train_df.TARGET.values[:-self.prediction_window]),\n",
        "              \"feat_dynamic_real\": [train_DHI, train_DNI, train_RH, train_T]\n",
        "              }],\n",
        "            freq=\"30min\",\n",
        "            one_dim_target=True\n",
        "        )\n",
        "\n",
        "        # TODO: Is valid set configured correctly?\n",
        "        valid_DHI = self.valid_df.DHI.values[:-self.prediction_window]\n",
        "        valid_DNI = self.valid_df.DNI.values[:-self.prediction_window]\n",
        "        valid_RH = self.valid_df.RH.values[:-self.prediction_window]\n",
        "        valid_T = self.valid_df['T'][:-self.prediction_window].values\n",
        "\n",
        "        self.valid_ds = ListDataset(\n",
        "            [{\"start\": self.valid_df.index[0],\n",
        "              \"target\": np.array(self.valid_df.TARGET.values[:-self.prediction_window]),\n",
        "              \"feat_dynamic_real\": [valid_DHI, valid_DNI, valid_RH, valid_T]\n",
        "              }],\n",
        "            freq=\"30min\",\n",
        "            one_dim_target=True\n",
        "        )\n",
        "\n",
        "    # calculate the sum of quantile loss from given period\n",
        "    # quantile forecast values and true values must be entered. Prediction is made in this method\n",
        "    def forecast_quantiles(self,\n",
        "                           dataset: Dataset,\n",
        "                           predictor: Predictor,\n",
        "                           num_samples: int = 100,\n",
        "                           prediction_window: int = 2*48) -> pd.DataFrame:\n",
        "        quantile_forecasts = {}\n",
        "        quantiles = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "\n",
        "        forecast_iter, ts_iter = make_evaluation_predictions(dataset, predictor=predictor, num_samples=num_samples)\n",
        "\n",
        "        forecasts = next(forecast_iter)  # forecasts: instance from SampleForecast\n",
        "        tss = next(ts_iter)  # tss : instance from pd.DataFrame\n",
        "\n",
        "        for quantile in quantiles:\n",
        "            quantile_forecasts[quantile] = forecasts.quantile(quantile)\n",
        "\n",
        "        return pd.DataFrame(quantile_forecasts, columns=quantiles, index=tss.index[-self.prediction_window:])\n",
        "\n",
        "    # deepAR model for bayesian optimization.\n",
        "    # This returns the sum of quantile loss for given parameters selected by bayesian optimizer\n",
        "    def model(self,\n",
        "              epochs,\n",
        "              context_length,\n",
        "              num_cells,\n",
        "              num_layers\n",
        "              ) -> float:\n",
        "\n",
        "        estimator_params = {\n",
        "            'cell_type': 'lstm',\n",
        "            'context_length': int(context_length),\n",
        "            'num_cells': int(num_cells),\n",
        "            'num_layers': int(num_layers),\n",
        "            'use_feat_dynamic_real': self.use_feat_dynamic_real,\n",
        "            'epochs': int(epochs)\n",
        "        }\n",
        "\n",
        "        trainer = Trainer(epochs=int(epochs),\n",
        "                          batch_size=self.batch_size,\n",
        "                          ctx=self.ctx,\n",
        "                          learning_rate=self.learning_rate)\n",
        "\n",
        "        estimator = DeepAREstimator(estimator_params, trainer=trainer, freq='30min',\n",
        "                                    prediction_length=self.prediction_window)\n",
        "\n",
        "        predictor = estimator.train(training_data=self.train_ds)\n",
        "\n",
        "        # TODO: maybe backtest_metrics can be used to shorten the process below\n",
        "        forecast_df = self.forecast_quantiles(self.valid_ds, predictor, 200)\n",
        "        forecast_df = refine_forecasts(forecast_df)\n",
        "        y_true = self.valid_df.TARGET[-self.prediction_window:]\n",
        "        quantile_loss = self.quantile_loss(y_true, forecast_df)\n",
        "\n",
        "        # record inserting\n",
        "        iter_record = pd.DataFrame(estimator_params, index=[self.internal_iter_num])\n",
        "        iter_record['epochs'] = int(epochs)\n",
        "        iter_record['batch_size'] = self.batch_size\n",
        "        iter_record['learning_rate'] = round(self.learning_rate, 4)\n",
        "        iter_record['quantile_loss'] = round(quantile_loss)\n",
        "\n",
        "        self.internal_iter_num += 1\n",
        "        self._records.append(iter_record)\n",
        "\n",
        "        # As bayesian optimizer tries to maximize the target value\n",
        "        # to make the model work, we need to inverse the sign so that it minimizes the loss\n",
        "        return -quantile_loss\n",
        "\n",
        "    # call this method when you actually tune\n",
        "    def tune_model(self,\n",
        "                   verbose: int = 2,\n",
        "                   init_points: int = 4,\n",
        "                   n_iter: int = 20,\n",
        "                   saving_folder: Optional[str] = None,\n",
        "                   skip_tune: bool = False,\n",
        "                   **kwargs):\n",
        "        best_params = None\n",
        "\n",
        "        # when you want to tune the model\n",
        "        if not skip_tune:\n",
        "            deepAR = BayesianOptimization(f=self.model, pbounds=self.pbounds, verbose=verbose)\n",
        "            deepAR.maximize(init_points=init_points, n_iter=n_iter)\n",
        "            print('best_target_value:', -deepAR.max['target'])\n",
        "            self._best_loss = -deepAR.max['target']\n",
        "\n",
        "            trainer = Trainer(epochs=int(deepAR.max['params']['epochs']),\n",
        "                              batch_size=self.batch_size,\n",
        "                              ctx=self.ctx,\n",
        "                              learning_rate=self.learning_rate)\n",
        "\n",
        "            estimator = DeepAREstimator(deepAR.max['params'], trainer=trainer,\n",
        "                                        freq='30min', prediction_length=self.prediction_window,\n",
        "                                        cell_type='lstm', use_feat_dynamic_real=self.use_feat_dynamic_real)\n",
        "            best_params = deepAR.max[\"params\"]\n",
        "\n",
        "            predictor = estimator.train(training_data=self.train_ds)\n",
        "\n",
        "            self._estimator = estimator\n",
        "            self._predictor = predictor\n",
        "\n",
        "        # when you do not want to tune the model but train with give parameter(**kwargs)\n",
        "        else:\n",
        "            print('Only training without tuning process...')\n",
        "            best_params = kwargs\n",
        "            trainer = Trainer(epochs=int(kwargs['epochs']),\n",
        "                              batch_size=self.batch_size, ctx=self.ctx,\n",
        "                              learning_rate=self.learning_rate)\n",
        "\n",
        "            kwargs.pop('epochs', None)\n",
        "            estimator = DeepAREstimator(**kwargs, trainer=trainer,\n",
        "                                        freq='30min', prediction_length=self.prediction_window,\n",
        "                                        cell_type='lstm', use_feat_dynamic_real=self.use_feat_dynamic_real)\n",
        "\n",
        "            predictor = estimator.train(training_data=self.train_ds)\n",
        "\n",
        "            self._estimator = estimator\n",
        "            self._predictor = predictor\n",
        "\n",
        "        print(\"Evaluating on valid set...\")\n",
        "        forecast_df = self.forecast_quantiles(self.valid_ds, predictor, 200)\n",
        "        forecast_df = refine_forecasts(forecast_df)\n",
        "\n",
        "        y_true = self.valid_df.TARGET[-self.prediction_window:]\n",
        "\n",
        "        quantile_sum = self.quantile_loss(y_true, forecast_df)\n",
        "\n",
        "        print(f'The lowest sum of quantile loss for validation set is {round(quantile_sum, 4)}'\n",
        "              f' with parameters {best_params}')\n",
        "\n",
        "        # Model saving process\n",
        "        if not saving_folder:\n",
        "            curpath = os.getcwd()\n",
        "            saving_folder = curpath + '/saved_model/model_' + str(round(quantile_sum, 4))\n",
        "\n",
        "        else:\n",
        "            saving_folder = saving_folder + '/model_' + str(round(quantile_sum, 4))\n",
        "\n",
        "        # model saving\n",
        "        try:\n",
        "            print(\"Saving the model under \" + saving_folder + \" with records.\")\n",
        "            Path(saving_folder).mkdir(parents=True)\n",
        "            self.current_saving_folder = saving_folder\n",
        "            predictor.serialize(Path(saving_folder))\n",
        "            record = self.return_records()\n",
        "            record.to_csv(saving_folder + '/optimizer_record.csv')\n",
        "            print(\"Successfully saved model.\")\n",
        "\n",
        "        except FileExistsError:\n",
        "            warnings.warn(f\"File or directory already exists in {saving_folder}\")\n",
        "\n",
        "        except:\n",
        "            warnings.warn(\"Saving file failed due to unknown reason. \"\n",
        "                          \"High probability of collision in predictor serialization is assumed\")\n",
        "\n",
        "        # to loads it back,\n",
        "        # from gluonts.model.predictor import Predictor\n",
        "        # predictor_deserialized = Predictor.deserialize(Path(\"/tmp/\"))\n",
        "\n",
        "\n",
        "    def predict_on_test(self, test_path: str,\n",
        "                        **kwargs) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "         test_path : str => path of directory or folder containing all the test files\n",
        "\n",
        "        Inner work\n",
        "        ----------\n",
        "         reads each test csv and converts it to ListDataset.\n",
        "         # TODO: is the test_data set is correctly configured?\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "         DataFrame containing quantile forecasts\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        #test_df = make_features(test_path)\n",
        "        # if timestamped csv files under timestamped folder are not available,\n",
        "        # it creates the new one using the test csv given from Dacon\n",
        "        test_stamper = Timestamper(test_path=test_path)\n",
        "        timestamped_path = test_stamper.stamp()\n",
        "        all_quantile_forecasts = []\n",
        "\n",
        "        for file_num in range(0, 81):\n",
        "            current_file = f'/{file_num}.csv'\n",
        "            test_df = pd.read_csv(timestamped_path + current_file)\n",
        "            test_df = make_features(test_df)\n",
        "            test_df['Timestamp'] = pd.to_datetime(test_df['Timestamp'])\n",
        "            test_df = test_df.set_index(test_df['Timestamp'])\n",
        "\n",
        "            test_range = pd.date_range(test_df.index[0], periods=48 * 9, freq='30min')\n",
        "            test_df.index = test_range\n",
        "\n",
        "\n",
        "            test_ds = ListDataset(\n",
        "                [{\"start\": test_df.index[0],\n",
        "                  \"target\": test_df.TARGET.values,\n",
        "                  \"feat_dynamic_real\": [test_df.DHI.values, test_df.DNI.values, test_df.RH.values,\n",
        "                                        test_df['T'].values]\n",
        "                  }],\n",
        "                freq=\"30min\",\n",
        "                one_dim_target=True\n",
        "            )\n",
        "\n",
        "            forecast_df = self.forecast_quantiles(test_ds, self._predictor, 200)\n",
        "            forecast_df = refine_forecasts(forecast_df)\n",
        "            forecast_df = pd.DataFrame(forecast_df.values, columns=forecast_df.columns)\n",
        "\n",
        "            all_quantile_forecasts.append(forecast_df)\n",
        "\n",
        "        final = pd.concat(all_quantile_forecasts, axis=0)\n",
        "        final[final < 0] = 0\n",
        "\n",
        "        self.make_submission(final, **kwargs)\n",
        "\n",
        "        return final\n",
        "\n",
        "    def make_submission(self, target_df: pd.DataFrame,\n",
        "                        sample_submission_file_path: Optional[str] = None) -> None:\n",
        "        cur_path = os.getcwd()\n",
        "\n",
        "        if not sample_submission_file_path:\n",
        "            sample_submission_file_path = cur_path + '/sample_submission.csv'\n",
        "\n",
        "        try:\n",
        "            sample_sub = pd.read_csv(sample_submission_file_path, index_col=0)\n",
        "            target_df.index = sample_sub.index\n",
        "            target_df.columns = sample_sub.columns\n",
        "\n",
        "        except:\n",
        "            warnings.warn(f\"Could not read sample submission file from {sample_submission_file_path}.\")\n",
        "\n",
        "        try:\n",
        "            target_df.to_csv(self.current_saving_folder)\n",
        "            print(\"Submission file is successfully save into \" + self.current_saving_folder)\n",
        "\n",
        "        except:\n",
        "            warnings.warn(f\"Could not save the submission file to {self.current_saving_folder}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPxaFOpmER1x"
      },
      "source": [
        "## 코드 끝"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLUS3Ciu5Hk4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/태양열 발전량 예측/Timestamped.csv\")\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "df = df.set_index(df['Timestamp'])\n",
        "\n",
        "cut_edge = pd.to_datetime('2018-11-30 23:30:00') # 2년치 학습\n",
        "temp = df[['TARGET', 'DHI', 'DNI', 'RH', 'T']]\n",
        "\n",
        "valid_start = pd.to_datetime('2018-12-03 00:00:00')\n",
        "valid_end = pd.to_datetime('2018-12-12 23:30:00')\n",
        "\n",
        "train = temp[:cut_edge]\n",
        "valid = temp[valid_start:valid_end]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqqAQQQu5NWO"
      },
      "source": [
        "pbounds = {'epochs': (50, 51),\n",
        "            'context_length': (48 * 2, 48 * 2 + 30),\n",
        "            'num_cells': (20, 60),\n",
        "            'num_layers': (2, 5)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN6Gzo0vfIlC"
      },
      "source": [
        "params = {'epochs': 40,\n",
        "            'context_length': 48 * 2,\n",
        "            'num_cells': 40,\n",
        "            'num_layers': 2}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HuHznrT4_Ko",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2d153f4-a623-4c61-ea7b-b9b4dd743e9e"
      },
      "source": [
        "saving_folder_path = '/content/drive/MyDrive/Colab Notebooks/태양열 발전량 예측/saved_model'\n",
        "sample_submission_file = '/content/drive/MyDrive/Colab Notebooks/태양열 발전량 예측/sample_submission.csv'\n",
        "test_file_path = '/content/drive/MyDrive/Colab Notebooks/태양열 발전량 예측/test'\n",
        "\n",
        "tuner = DeepARTuner(train, valid, pbounds=pbounds, learning_rate=0.001, batch_size=64, use_feat_dynamic_real=True)\n",
        "\n",
        "tuner.tune_model(n_iter=10, saving_folder=saving_folder_path, skip_tune= False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | contex... |  epochs   | num_cells | num_la... |\n",
            "-------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8JBrnYb4kCX"
      },
      "source": [
        "print(\"\\nTuner now tries to predict on test_set...\")\n",
        "submission = tuner.predict_on_test(test_path=test_file_path, \n",
        "                                   sample_submission_file_path=sample_submission_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87VJS5Mpr0SJ"
      },
      "source": [
        "submission.to_csv('/content/drive/MyDrive/Colab Notebooks/태양열 발전량 예측/submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC3CubGLsKeI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}